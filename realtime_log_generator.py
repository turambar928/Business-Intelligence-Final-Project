#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶æ–°é—»æ—¥å¿—ç”Ÿæˆå™¨
ä»PENSæ•°æ®é›†è¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œç”Ÿæˆç¬¦åˆæ–°æ—¥å¿—æ ¼å¼çš„å®æ—¶æ—¥å¿—æ–‡ä»¶
"""

import json
import time
import random
import mysql.connector
from datetime import datetime, timedelta
import threading
import os
import logging
from typing import Dict, List, Any
import uuid

class RealTimeLogGenerator:
    """å®æ—¶æ—¥å¿—ç”Ÿæˆå™¨"""
    
    def __init__(self, mysql_config: Dict[str, Any], output_dir: str = "web_logs"):
        self.mysql_config = mysql_config
        self.output_dir = output_dir
        self.logger = logging.getLogger(__name__)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”¨æˆ·ä¼šè¯ç¼“å­˜
        self.user_sessions = {}
        self.user_interests = {}
        
        # PENSæ•°æ®ç¼“å­˜
        self.pens_data = []
        self.current_index = 0
        
        # æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå‚æ•°
        self.action_types = ["read", "skip", "share", "like", "comment", "click"]
        self.device_types = ["mobile", "desktop", "tablet"]
        self.os_types = ["iOS", "Android", "Windows", "Mac"]
        self.browsers = ["Chrome", "Safari", "Firefox", "Edge"]
        self.sources = ["news_app", "web_portal", "mobile_app"]
        
        # ä¸­å›½çœå¸‚æ•°æ®
        self.locations = [
            {"country": "China", "province": "Beijing", "city": "Beijing"},
            {"country": "China", "province": "Shanghai", "city": "Shanghai"},
            {"country": "China", "province": "Guangdong", "city": "Guangzhou"},
            {"country": "China", "province": "Guangdong", "city": "Shenzhen"},
            {"country": "China", "province": "Zhejiang", "city": "Hangzhou"},
            {"country": "China", "province": "Jiangsu", "city": "Nanjing"},
            {"country": "China", "province": "Sichuan", "city": "Chengdu"},
            {"country": "China", "province": "Hubei", "city": "Wuhan"},
            {"country": "China", "province": "Shaanxi", "city": "Xi'an"},
            {"country": "China", "province": "Tianjin", "city": "Tianjin"}
        ]
        
        # ç”¨æˆ·å…´è¶£æ ‡ç­¾
        self.interest_categories = {
            "sports": ["ä½“è‚²", "è¶³çƒ", "ç¯®çƒ", "ç½‘çƒ", "å¥¥è¿ä¼š"],
            "news": ["æ–°é—»", "æ”¿æ²»", "ç¤¾ä¼š", "å›½é™…"],
            "technology": ["ç§‘æŠ€", "AI", "äº’è”ç½‘", "æ‰‹æœº", "ç”µè„‘"],
            "business": ["å•†ä¸š", "ç»æµ", "é‡‘è", "è‚¡ç¥¨", "æŠ•èµ„"],
            "entertainment": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ˜æ˜Ÿ", "ç”µè§†"],
            "lifestyle": ["ç”Ÿæ´»", "å¥åº·", "ç¾é£Ÿ", "æ—…è¡Œ", "æ—¶å°š"],
            "finance": ["è´¢ç»", "ç†è´¢", "é“¶è¡Œ", "ä¿é™©"]
        }
        
    def connect_mysql(self):
        """è¿æ¥MySQLæ•°æ®åº“"""
        try:
            return mysql.connector.connect(**self.mysql_config)
        except Exception as e:
            self.logger.error(f"MySQLè¿æ¥å¤±è´¥: {e}")
            return None
    
    def load_pens_data(self, file_path: str, max_records: int = 10000):
        """åŠ è½½PENSæ•°æ®é›†"""
        self.logger.info(f"å¼€å§‹åŠ è½½PENSæ•°æ®: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                # è·³è¿‡æ ‡é¢˜è¡Œ
                header = file.readline().strip().split('\t')
                self.logger.info(f"æ•°æ®åˆ—: {header}")
                
                count = 0
                for line in file:
                    if count >= max_records:
                        break
                        
                    parts = line.strip().split('\t')
                    if len(parts) >= 4:
                        user_id = parts[0]
                        clicked_news = parts[1].split() if len(parts) > 1 else []
                        dwell_times = list(map(int, parts[2].split())) if len(parts) > 2 and parts[2] else []
                        exposure_times = parts[3] if len(parts) > 3 else ""
                        
                        # è§£ææ›å…‰æ—¶é—´
                        time_stamps = []
                        if exposure_times:
                            time_parts = exposure_times.split('#TAB#')
                            for time_part in time_parts:
                                try:
                                    time_stamps.append(datetime.strptime(time_part.strip(), '%m/%d/%Y %I:%M:%S %p'))
                                except:
                                    continue
                        
                        self.pens_data.append({
                            'user_id': user_id,
                            'clicked_news': clicked_news,
                            'dwell_times': dwell_times,
                            'time_stamps': time_stamps
                        })
                        count += 1
                        
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(self.pens_data)} æ¡PENSè®°å½•")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½PENSæ•°æ®å¤±è´¥: {e}")
    
    def get_news_details(self, news_id: str) -> Dict[str, Any]:
        """ä»æ•°æ®åº“è·å–æ–°é—»è¯¦æƒ…"""
        conn = self.connect_mysql()
        if not conn:
            return self._generate_mock_news_details(news_id)
        
        try:
            cursor = conn.cursor(dictionary=True)
            cursor.execute("""
                SELECT * FROM news_articles WHERE news_id = %s LIMIT 1
            """, (news_id,))
            
            result = cursor.fetchone()
            if result:
                return {
                    "category": result.get('category', 'news'),
                    "topic": result.get('topic', 'general'),
                    "headline": result.get('headline', 'Sample News Headline'),
                    "content": result.get('content', 'Sample news content...'),
                    "word_count": result.get('word_count', 100),
                    "publish_time": result.get('publish_time', datetime.now()).isoformat() + 'Z',
                    "source": result.get('source', 'news_source'),
                    "tags": json.loads(result.get('tags', '[]')),
                    "entities": json.loads(result.get('entities', '{}'))
                }
            else:
                return self._generate_mock_news_details(news_id)
                
        except Exception as e:
            self.logger.warning(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥: {e}")
            return self._generate_mock_news_details(news_id)
        finally:
            if conn:
                cursor.close()
                conn.close()
    
    def _generate_mock_news_details(self, news_id: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ–°é—»è¯¦æƒ…"""
        categories = list(self.interest_categories.keys())
        category = random.choice(categories)
        
        return {
            "category": category,
            "topic": random.choice(["breaking", "analysis", "report", "update"]),
            "headline": f"Sample News Headline for {news_id}",
            "content": "This is a sample news content for simulation purposes.",
            "word_count": random.randint(100, 1000),
            "publish_time": (datetime.now() - timedelta(hours=random.randint(1, 24))).isoformat() + 'Z',
            "source": random.choice(["CNN", "BBC", "Reuters", "AP"]),
            "tags": random.sample(self.interest_categories[category], k=min(3, len(self.interest_categories[category]))),
            "entities": {
                "persons": [],
                "organizations": [],
                "locations": []
            }
        }
    
    def get_user_session(self, user_id: str) -> str:
        """è·å–æˆ–ç”Ÿæˆç”¨æˆ·ä¼šè¯ID"""
        if user_id not in self.user_sessions:
            self.user_sessions[user_id] = f"session_{uuid.uuid4().hex[:12]}"
        return self.user_sessions[user_id]
    
    def get_user_interests(self, user_id: str) -> List[str]:
        """è·å–æˆ–ç”Ÿæˆç”¨æˆ·å…´è¶£"""
        if user_id not in self.user_interests:
            # éšæœºé€‰æ‹©2-4ä¸ªå…´è¶£ç±»åˆ«
            categories = random.sample(list(self.interest_categories.keys()), k=random.randint(2, 4))
            interests = []
            for category in categories:
                interests.extend(random.sample(self.interest_categories[category], k=random.randint(1, 2)))
            self.user_interests[user_id] = interests
        return self.user_interests[user_id]
    
    def get_time_of_day(self, timestamp: datetime) -> str:
        """æ ¹æ®æ—¶é—´åˆ¤æ–­æ—¶æ®µ"""
        hour = timestamp.hour
        if 5 <= hour < 12:
            return "morning"
        elif 12 <= hour < 18:
            return "afternoon"
        elif 18 <= hour < 22:
            return "evening"
        else:
            return "night"
    
    def generate_ai_analysis(self, news_details: Dict[str, Any], action_type: str, dwell_time: int) -> Dict[str, Any]:
        """ç”ŸæˆAIåˆ†ææ•°æ®"""
        return {
            "sentiment_score": round(random.uniform(-1.0, 1.0), 2),
            "topic_keywords": news_details.get("tags", [])[:3],
            "viral_potential": round(random.uniform(0.0, 1.0), 2),
            "trend_score": round(random.uniform(0.0, 1.0), 2),
            "recommendation_relevance": round(random.uniform(0.0, 1.0), 2),
            "content_quality": round(random.uniform(0.0, 1.0), 2),
            "user_profile_match": round(random.uniform(0.0, 1.0), 2)
        }
    
    def generate_log_entry(self, user_data: Dict[str, Any], news_index: int) -> Dict[str, Any]:
        """ç”Ÿæˆå•æ¡æ—¥å¿—è®°å½•"""
        user_id = user_data['user_id']
        clicked_news = user_data['clicked_news']
        dwell_times = user_data['dwell_times']
        time_stamps = user_data['time_stamps']
        
        # å¦‚æœæ²¡æœ‰ç‚¹å‡»æ–°é—»ï¼Œè·³è¿‡
        if not clicked_news or news_index >= len(clicked_news):
            return None
        
        news_id = clicked_news[news_index]
        dwell_time = dwell_times[news_index] if news_index < len(dwell_times) else random.randint(10, 300)
        
        # ä½¿ç”¨åŸå§‹æ—¶é—´æˆ³æˆ–ç”Ÿæˆæ–°çš„
        if news_index < len(time_stamps):
            timestamp = time_stamps[news_index]
        else:
            timestamp = datetime.now()
        
        # è·å–æ–°é—»è¯¦æƒ…
        news_details = self.get_news_details(news_id)
        
        # ç”Ÿæˆæ—¥å¿—è®°å½•
        action_type = random.choice(self.action_types)
        device_info = {
            "device_type": random.choice(self.device_types),
            "os": random.choice(self.os_types),
            "browser": random.choice(self.browsers)
        }
        
        log_entry = {
            "timestamp": timestamp.isoformat() + 'Z',
            "user_id": user_id,
            "news_id": news_id,
            "action_type": action_type,
            "session_id": self.get_user_session(user_id),
            "dwell_time": dwell_time,
            "device_info": device_info,
            "location": random.choice(self.locations),
            "news_details": news_details,
            "user_context": {
                "previous_articles": clicked_news[max(0, news_index-2):news_index],
                "reading_time_of_day": self.get_time_of_day(timestamp),
                "is_weekend": timestamp.weekday() >= 5,
                "user_interests": self.get_user_interests(user_id),
                "engagement_score": round(random.uniform(0.0, 1.0), 2)
            },
            "interaction_data": {
                "scroll_depth": round(random.uniform(0.1, 1.0), 2),
                "clicks_count": random.randint(0, 10),
                "shares_count": random.randint(0, 3),
                "comments_count": random.randint(0, 5),
                "likes_count": random.randint(0, 8)
            },
            "ai_analysis": self.generate_ai_analysis(news_details, action_type, dwell_time)
        }
        
        return log_entry
    
    def generate_batch_log(self, batch_size: int = 10) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰¹é‡æ—¥å¿—"""
        events = []
        
        for _ in range(batch_size):
            if not self.pens_data:
                break
                
            # å¾ªç¯ä½¿ç”¨PENSæ•°æ®
            user_data = self.pens_data[self.current_index % len(self.pens_data)]
            
            # éšæœºé€‰æ‹©è¯¥ç”¨æˆ·çš„ä¸€ä¸ªæ–°é—»
            if user_data['clicked_news']:
                news_index = random.randint(0, len(user_data['clicked_news']) - 1)
                log_entry = self.generate_log_entry(user_data, news_index)
                if log_entry:
                    events.append(log_entry)
            
            self.current_index += 1
        
        batch_log = {
            "batch_id": f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}",
            "events": events,
            "metadata": {
                "source": random.choice(self.sources),
                "version": "1.0",
                "processed_time": datetime.now().isoformat() + 'Z'
            }
        }
        
        return batch_log
    
    def save_log_to_file(self, log_data: Dict[str, Any], filename: str = None):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"news_logs_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ—¥å¿—å·²ä¿å­˜åˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def start_realtime_generation(self, interval_seconds: int = 5, batch_size: int = 10):
        """å¼€å§‹å®æ—¶ç”Ÿæˆæ—¥å¿—"""
        self.logger.info(f"å¼€å§‹å®æ—¶æ—¥å¿—ç”Ÿæˆï¼Œé—´éš”: {interval_seconds}ç§’ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        try:
            while True:
                # ç”Ÿæˆæ‰¹é‡æ—¥å¿—
                batch_log = self.generate_batch_log(batch_size)
                
                if batch_log['events']:
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    self.save_log_to_file(batch_log)
                    self.logger.info(f"ç”Ÿæˆäº† {len(batch_log['events'])} æ¡æ—¥å¿—è®°å½•")
                else:
                    self.logger.warning("æ²¡æœ‰ç”Ÿæˆä»»ä½•æ—¥å¿—è®°å½•")
                
                # ç­‰å¾…æŒ‡å®šé—´éš”
                time.sleep(interval_seconds)
                
        except KeyboardInterrupt:
            self.logger.info("æ—¥å¿—ç”Ÿæˆå·²åœæ­¢")
        except Exception as e:
            self.logger.error(f"æ—¥å¿—ç”Ÿæˆå‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å®æ—¶æ–°é—»æ—¥å¿—ç”Ÿæˆå™¨")
    print("="*50)
    
    # æ•°æ®åº“é…ç½®
    mysql_config = {
        'host': 'localhost',
        'user': 'analytics_user',
        'password': 'pass123',
        'database': 'news_analytics',
        'charset': 'utf8mb4'
    }
    
    # åˆ›å»ºæ—¥å¿—ç”Ÿæˆå™¨
    generator = RealTimeLogGenerator(mysql_config, "web_logs")
    
    # åŠ è½½PENSæ•°æ®
    print("ğŸ“ åŠ è½½PENSæ•°æ®é›†...")
    generator.load_pens_data("data/PENS/train.tsv", max_records=5000)  # é™åˆ¶è®°å½•æ•°ä»¥ä¾¿æµ‹è¯•
    
    if not generator.pens_data:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°PENSæ•°æ®ï¼Œé€€å‡ºç¨‹åº")
        return
    
    print(f"âœ… å·²åŠ è½½ {len(generator.pens_data)} æ¡ç”¨æˆ·è¡Œä¸ºè®°å½•")
    
    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        interval = int(input("è¯·è¾“å…¥ç”Ÿæˆé—´éš”ç§’æ•° (é»˜è®¤5ç§’): ") or "5")
        batch_size = int(input("è¯·è¾“å…¥æ¯æ‰¹æ¬¡è®°å½•æ•° (é»˜è®¤10æ¡): ") or "10")
    except ValueError:
        interval = 5
        batch_size = 10
    
    print(f"\nğŸš€ å¼€å§‹å®æ—¶ç”Ÿæˆæ—¥å¿—...")
    print(f"   é—´éš”æ—¶é—´: {interval} ç§’")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size} æ¡")
    print(f"   è¾“å‡ºç›®å½•: web_logs/")
    print("   æŒ‰ Ctrl+C åœæ­¢ç”Ÿæˆ")
    print("-" * 50)
    
    # å¼€å§‹å®æ—¶ç”Ÿæˆ
    generator.start_realtime_generation(interval, batch_size)

if __name__ == "__main__":
    main()