#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„Sparkæµå¤„ç†åˆ†æå™¨
æ”¯æŒæ–°æ—¥å¿—æ ¼å¼å’Œä¸ƒå¤§åˆ†æåŠŸèƒ½
"""

import json
import logging
from datetime import datetime
from typing import Dict, Any, List
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, window, count, avg, max as spark_max, 
    min as spark_min, sum as spark_sum, collect_list, 
    when, regexp_extract, length, split, explode,
    current_timestamp, date_format, hour, dayofweek, countDistinct
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, BooleanType, TimestampType, ArrayType
)
from src.enhanced_ai_analysis_engine import EnhancedAIAnalysisEngine

class EnhancedNewsStreamingAnalyzer:
    """å¢å¼ºçš„æ–°é—»æµå¤„ç†åˆ†æå™¨"""
    
    def __init__(self, kafka_config: Dict[str, Any], mysql_config: Dict[str, Any]):
        self.kafka_config = kafka_config
        self.mysql_config = mysql_config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–Spark
        self.spark = self._create_spark_session()
        
        # åˆå§‹åŒ–AIåˆ†æå¼•æ“
        self.ai_engine = EnhancedAIAnalysisEngine(mysql_config)
        
        # å®šä¹‰å¢å¼ºçš„æ•°æ®æ¨¡å¼
        self.schema = self._define_enhanced_schema()
        
        # æ‰¹æ¬¡è®¡æ•°å™¨
        self.batch_count = 0
    
    def _create_spark_session(self) -> SparkSession:
        """åˆ›å»ºSparkä¼šè¯"""
        return SparkSession.builder \
            .appName("EnhancedNewsStreamingAnalyzer") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
            .getOrCreate()
    
    def _define_enhanced_schema(self) -> StructType:
        """å®šä¹‰å¢å¼ºçš„æ•°æ®æ¨¡å¼"""
        return StructType([
            StructField("timestamp", StringType(), True),
            StructField("user_id", StringType(), True),
            StructField("news_id", StringType(), True),
            StructField("action_type", StringType(), True),
            StructField("session_id", StringType(), True),
            StructField("dwell_time", IntegerType(), True),
            StructField("device_info", StructType([
                StructField("device_type", StringType(), True),
                StructField("os", StringType(), True),
                StructField("browser", StringType(), True)
            ]), True),
            StructField("location", StructType([
                StructField("country", StringType(), True),
                StructField("province", StringType(), True),
                StructField("city", StringType(), True)
            ]), True),
            StructField("news_details", StructType([
                StructField("category", StringType(), True),
                StructField("topic", StringType(), True),
                StructField("headline", StringType(), True),
                StructField("content", StringType(), True),
                StructField("word_count", IntegerType(), True),
                StructField("publish_time", StringType(), True),
                StructField("source", StringType(), True),
                StructField("tags", ArrayType(StringType()), True),
                StructField("entities", StructType([
                    StructField("persons", ArrayType(StringType()), True),
                    StructField("organizations", ArrayType(StringType()), True),
                    StructField("locations", ArrayType(StringType()), True)
                ]), True)
            ]), True),
            StructField("user_context", StructType([
                StructField("previous_articles", ArrayType(StringType()), True),
                StructField("reading_time_of_day", StringType(), True),
                StructField("is_weekend", BooleanType(), True),
                StructField("user_interests", ArrayType(StringType()), True),
                StructField("engagement_score", DoubleType(), True)
            ]), True),
            StructField("interaction_data", StructType([
                StructField("scroll_depth", DoubleType(), True),
                StructField("clicks_count", IntegerType(), True),
                StructField("shares_count", IntegerType(), True),
                StructField("comments_count", IntegerType(), True),
                StructField("likes_count", IntegerType(), True)
            ]), True)
        ])
    
    def start_streaming(self):
        """å¯åŠ¨æµå¤„ç†"""
        self.logger.info("å¯åŠ¨å¢å¼ºçš„æ–°é—»æµå¤„ç†åˆ†æå™¨...")
        
        # è¯»å–Kafkaæµ
        df = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", self.kafka_config['bootstrap_servers']) \
            .option("subscribe", self.kafka_config['topic']) \
            .option("startingOffsets", "latest") \
            .load()
        
        # è§£æJSONæ•°æ®
        parsed_df = df.select(
            from_json(col("value").cast("string"), self.schema).alias("data")
        ).select("data.*")
        
        # å¯åŠ¨å¤šä¸ªåˆ†ææŸ¥è¯¢
        queries = []
        
        # 1. å®æ—¶æ–°é—»çƒ­åº¦åˆ†æ
        news_popularity_query = self._create_news_popularity_query(parsed_df)
        queries.append(news_popularity_query)
        
        # 2. ç”¨æˆ·è¡Œä¸ºåˆ†æ
        user_behavior_query = self._create_user_behavior_query(parsed_df)
        queries.append(user_behavior_query)
        
        # 3. åˆ†ç±»è¶‹åŠ¿åˆ†æ
        category_trend_query = self._create_category_trend_query(parsed_df)
        queries.append(category_trend_query)
        
        # 4. ä¸»è¦AIåˆ†æå¤„ç†
        ai_analysis_query = self._create_ai_analysis_query(parsed_df)
        queries.append(ai_analysis_query)
        
        # ç­‰å¾…æ‰€æœ‰æŸ¥è¯¢å®Œæˆ
        try:
            for query in queries:
                query.awaitTermination()
        except KeyboardInterrupt:
            self.logger.info("åœæ­¢æµå¤„ç†...")
            for query in queries:
                query.stop()
        finally:
            self.spark.stop()
    
    def _create_news_popularity_query(self, df):
        """åˆ›å»ºæ–°é—»çƒ­åº¦åˆ†ææŸ¥è¯¢"""
        news_popularity = df \
            .filter(col("action_type").isin(["read", "share", "like"])) \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "10 minutes"),
                col("news_id"),
                col("news_details.category"),
                col("news_details.topic")
            ) \
            .agg(
                count("*").alias("total_interactions"),
                countDistinct("user_id").alias("unique_users"),
                avg("dwell_time").alias("avg_dwell_time"),
                spark_sum(when(col("action_type") == "share", 1).otherwise(0)).alias("shares"),
                spark_sum(when(col("action_type") == "like", 1).otherwise(0)).alias("likes"),
                avg("user_context.engagement_score").alias("avg_engagement")
            )
        
        return news_popularity.writeStream \
            .outputMode("update") \
            .format("console") \
            .option("truncate", False) \
            .trigger(processingTime="30 seconds") \
            .start()
    
    def _create_user_behavior_query(self, df):
        """åˆ›å»ºç”¨æˆ·è¡Œä¸ºåˆ†ææŸ¥è¯¢"""
        user_behavior = df \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "15 minutes"),
                col("user_id"),
                col("device_info.device_type")
            ) \
            .agg(
                count("*").alias("total_actions"),
                countDistinct("news_id").alias("articles_read"),
                avg("dwell_time").alias("avg_reading_time"),
                avg("interaction_data.scroll_depth").alias("avg_scroll_depth"),
                spark_sum("interaction_data.clicks_count").alias("total_clicks"),
                avg("user_context.engagement_score").alias("engagement_score")
            )
        
        return user_behavior.writeStream \
            .outputMode("update") \
            .format("console") \
            .option("truncate", False) \
            .trigger(processingTime="45 seconds") \
            .start()
    
    def _create_category_trend_query(self, df):
        """åˆ›å»ºåˆ†ç±»è¶‹åŠ¿åˆ†ææŸ¥è¯¢"""
        category_trends = df \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "20 minutes"),
                col("news_details.category"),
                col("news_details.topic")
            ) \
            .agg(
                countDistinct("news_id").alias("unique_articles"),
                count("*").alias("total_interactions"),
                countDistinct("user_id").alias("unique_readers"),
                avg("user_context.engagement_score").alias("avg_engagement"),
                collect_list("news_details.tags").alias("all_tags")
            )
        
        return category_trends.writeStream \
            .outputMode("update") \
            .format("console") \
            .option("truncate", False) \
            .trigger(processingTime="60 seconds") \
            .start()
    
    def _create_ai_analysis_query(self, df):
        """åˆ›å»ºAIåˆ†ææŸ¥è¯¢"""
        def process_batch(batch_df, batch_id):
            """å¤„ç†æ¯ä¸ªæ‰¹æ¬¡è¿›è¡ŒAIåˆ†æ"""
            self.batch_count += 1
            start_time = datetime.now()
            
            self.logger.info(f"å¤„ç†æ‰¹æ¬¡ #{self.batch_count} (batch_id: {batch_id})")
            
            try:
                # æ”¶é›†æ‰¹æ¬¡æ•°æ®
                batch_data = batch_df.collect()
                
                if batch_data:
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    events = []
                    for row in batch_data:
                        event = self._row_to_dict(row)
                        events.append(event)
                    
                    self.logger.info(f"æ‰¹æ¬¡ #{self.batch_count} åŒ…å« {len(events)} æ¡è®°å½•")
                    
                    # æ‰§è¡ŒAIåˆ†æ
                    analysis_results = self.ai_engine.analyze_batch(events)
                    
                    # è¾“å‡ºåˆ†æç»“æœæ‘˜è¦
                    self._print_analysis_summary(analysis_results)
                    
                    # ä¿å­˜åˆ°MySQL
                    self._save_batch_to_mysql(events, analysis_results)
                    
                    processing_time = (datetime.now() - start_time).total_seconds()
                    self.logger.info(f"æ‰¹æ¬¡ #{self.batch_count} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
                
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡ #{self.batch_count} å¤„ç†å¤±è´¥: {e}")
        
        return df.writeStream \
            .foreachBatch(process_batch) \
            .trigger(processingTime="10 seconds") \
            .start()
    
    def _row_to_dict(self, row) -> Dict[str, Any]:
        """å°†Spark Rowè½¬æ¢ä¸ºå­—å…¸"""
        try:
            return {
                'timestamp': row.timestamp,
                'user_id': row.user_id,
                'news_id': row.news_id,
                'action_type': row.action_type,
                'session_id': row.session_id,
                'dwell_time': row.dwell_time or 0,
                'device_info': {
                    'device_type': row.device_info.device_type if row.device_info else None,
                    'os': row.device_info.os if row.device_info else None,
                    'browser': row.device_info.browser if row.device_info else None
                } if row.device_info else {},
                'location': {
                    'country': row.location.country if row.location else None,
                    'province': row.location.province if row.location else None,
                    'city': row.location.city if row.location else None
                } if row.location else {},
                'news_details': {
                    'category': row.news_details.category if row.news_details else None,
                    'topic': row.news_details.topic if row.news_details else None,
                    'headline': row.news_details.headline if row.news_details else None,
                    'content': row.news_details.content if row.news_details else None,
                    'word_count': row.news_details.word_count if row.news_details else 0,
                    'publish_time': row.news_details.publish_time if row.news_details else None,
                    'source': row.news_details.source if row.news_details else None,
                    'tags': row.news_details.tags if row.news_details else [],
                    'entities': {
                        'persons': row.news_details.entities.persons if (row.news_details and row.news_details.entities) else [],
                        'organizations': row.news_details.entities.organizations if (row.news_details and row.news_details.entities) else [],
                        'locations': row.news_details.entities.locations if (row.news_details and row.news_details.entities) else []
                    } if (row.news_details and row.news_details.entities) else {}
                } if row.news_details else {},
                'user_context': {
                    'previous_articles': row.user_context.previous_articles if row.user_context else [],
                    'reading_time_of_day': row.user_context.reading_time_of_day if row.user_context else None,
                    'is_weekend': row.user_context.is_weekend if row.user_context else False,
                    'user_interests': row.user_context.user_interests if row.user_context else [],
                    'engagement_score': row.user_context.engagement_score if row.user_context else 0.0
                } if row.user_context else {},
                'interaction_data': {
                    'scroll_depth': row.interaction_data.scroll_depth if row.interaction_data else 0.0,
                    'clicks_count': row.interaction_data.clicks_count if row.interaction_data else 0,
                    'shares_count': row.interaction_data.shares_count if row.interaction_data else 0,
                    'comments_count': row.interaction_data.comments_count if row.interaction_data else 0,
                    'likes_count': row.interaction_data.likes_count if row.interaction_data else 0
                } if row.interaction_data else {}
            }
        except Exception as e:
            self.logger.error(f"è½¬æ¢è¡Œæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def _print_analysis_summary(self, results: Dict):
        """æ‰“å°åˆ†æç»“æœæ‘˜è¦"""
        print(f"\n{'='*50}")
        print(f"ğŸ“Š AIåˆ†æç»“æœæ‘˜è¦ - æ‰¹æ¬¡ #{self.batch_count}")
        print(f"{'='*50}")
        print(f"ğŸ•’ å¤„ç†æ—¶é—´: {results.get('timestamp', 'N/A')}")
        print(f"ğŸ“ å¤„ç†è®°å½•æ•°: {results.get('processed_count', 0)}")
        
        analysis = results.get('analysis_results', {})
        
        # æ–°é—»ç”Ÿå‘½å‘¨æœŸ
        lifecycle = analysis.get('news_lifecycle', [])
        print(f"ğŸ“ˆ æ–°é—»ç”Ÿå‘½å‘¨æœŸåˆ†æ: {len(lifecycle)} ç¯‡æ–°é—»")
        
        # åˆ†ç±»è¶‹åŠ¿
        trends = analysis.get('category_trends', [])
        print(f"ğŸ“Š åˆ†ç±»è¶‹åŠ¿åˆ†æ: {len(trends)} ä¸ªåˆ†ç±»")
        
        # ç”¨æˆ·å…´è¶£
        interests = analysis.get('user_interests', [])
        print(f"ğŸ‘¥ ç”¨æˆ·å…´è¶£åˆ†æ: {len(interests)} ä¸ªç”¨æˆ·")
        
        # ç—…æ¯’æ€§é¢„æµ‹
        viral = analysis.get('viral_predictions', [])
        high_viral = [v for v in viral if v.get('viral_score', 0) > 0.7]
        print(f"ğŸ”¥ çˆ†æ¬¾é¢„æµ‹: {len(high_viral)}/{len(viral)} ç¯‡é«˜ç—…æ¯’æ€§æ–°é—»")
        
        # æ¨è
        recommendations = analysis.get('recommendations', [])
        print(f"ğŸ’¡ ç”Ÿæˆæ¨è: {len(recommendations)} ä¸ªç”¨æˆ·")
        
        # æƒ…æ„Ÿåˆ†æ
        sentiments = analysis.get('sentiment_analysis', [])
        if sentiments:
            avg_sentiment = sum([s.get('sentiment_score', 0) for s in sentiments]) / len(sentiments)
            print(f"ğŸ˜Š æƒ…æ„Ÿåˆ†æ: å¹³å‡æƒ…æ„Ÿåˆ†æ•° {avg_sentiment:.3f}")
        
        print(f"{'='*50}\n")
    
    def _save_batch_to_mysql(self, events: List[Dict], analysis_results: Dict):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®åˆ°MySQL"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„MySQLä¿å­˜é€»è¾‘
            # ä¿å­˜åŸå§‹äº‹ä»¶å’Œåˆ†æç»“æœåˆ°ç›¸åº”çš„è¡¨
            pass
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ‰¹æ¬¡æ•°æ®åˆ°MySQLå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Kafkaé…ç½®
    kafka_config = {
        'bootstrap_servers': 'localhost:9092',
        'topic': 'news-events'
    }
    
    # MySQLé…ç½®  
    mysql_config = {
        'host': 'localhost',
        'user': 'root',
        'password': 'your_password',
        'database': 'news_analytics'
    }
    
    # åˆ›å»ºå¹¶å¯åŠ¨åˆ†æå™¨
    analyzer = EnhancedNewsStreamingAnalyzer(kafka_config, mysql_config)
    analyzer.start_streaming()

if __name__ == "__main__":
    main() 