#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PENSæ–°é—»æ•°æ®å¯¼å…¥è„šæœ¬
å°†data/PENS/news.tsvä¸­çš„æ‰€æœ‰æ–°é—»ä¿¡æ¯å¯¼å…¥åˆ°news_articlesè¡¨ä¸­
"""

import json
import re
import mysql.connector
from datetime import datetime, timedelta
import random
from typing import Dict, Any
import logging

class PENSNewsImporter:
    """PENSæ–°é—»æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, mysql_config: Dict[str, Any]):
        self.mysql_config = mysql_config
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # æ–°é—»æ¥æºåˆ—è¡¨
        self.sources = [
            "CNN", "BBC", "Reuters", "Associated Press", "New York Times", 
            "Washington Post", "The Guardian", "Fox News", "ESPN", "TechCrunch",
            "Forbes", "Bloomberg", "Wall Street Journal", "USA Today", "NBC News"
        ]
    
    def connect_mysql(self):
        """è¿æ¥MySQLæ•°æ®åº“"""
        try:
            return mysql.connector.connect(**self.mysql_config)
        except Exception as e:
            self.logger.error(f"MySQLè¿æ¥å¤±è´¥: {e}")
            return None
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text or text.strip() == '':
            return ''
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        if len(text) > 10000:
            text = text[:10000] + "..."
        
        return text
    
    def clean_headline(self, headline: str) -> str:
        """æ¸…ç†æ ‡é¢˜"""
        if not headline or headline.strip() == '':
            return 'Untitled News'
        
        headline = self.clean_text(headline)
        
        # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
        if len(headline) > 500:
            headline = headline[:500] + "..."
            
        return headline
    
    def parse_entities(self, title_entity: str, entity_content: str) -> str:
        """è§£æå®ä½“ä¿¡æ¯å¹¶è½¬æ¢ä¸ºJSONæ ¼å¼"""
        entities = {
            "persons": [],
            "organizations": [],
            "locations": []
        }
        
        try:
            if title_entity and title_entity.strip() and title_entity.strip() != '{}':
                # ç®€å•çš„å­—ç¬¦ä¸²è§£æï¼Œæå–å¼•å·å†…çš„å†…å®¹
                matches = re.findall(r'"([^"]*)"', title_entity)
                for match in matches:
                    if match and match.strip():
                        # ç®€å•åˆ¤æ–­å®ä½“ç±»å‹
                        if any(word in match.lower() for word in ['corp', 'inc', 'company', 'fc', 'united']):
                            entities["organizations"].append(match)
                        elif any(word in match.lower() for word in ['city', 'state', 'country', 'washington', 'atlanta']):
                            entities["locations"].append(match)
                        else:
                            entities["persons"].append(match)
            
        except Exception as e:
            self.logger.warning(f"å®ä½“è§£æå¤±è´¥: {e}")
        
        return json.dumps(entities, ensure_ascii=False)
    
    def generate_tags(self, headline: str, content: str, category: str, topic: str) -> str:
        """ç”Ÿæˆæ ‡ç­¾"""
        tags = set()
        
        # æ·»åŠ åˆ†ç±»å’Œä¸»é¢˜ä½œä¸ºæ ‡ç­¾
        if category:
            tags.add(category)
        if topic:
            tags.add(topic)
            
        # ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯ä½œä¸ºæ ‡ç­¾
        headline_lower = headline.lower() if headline else ''
        content_lower = content.lower() if content else ''
        
        # å¸¸è§çš„æ–°é—»å…³é”®è¯
        keywords = [
            'breaking', 'news', 'update', 'report', 'analysis', 'exclusive',
            'football', 'soccer', 'basketball', 'baseball', 'tennis', 'olympics',
            'politics', 'election', 'government', 'policy', 'congress', 'senate',
            'technology', 'ai', 'tech', 'innovation', 'startup', 'digital',
            'business', 'economy', 'market', 'finance', 'stock', 'investment',
            'health', 'medical', 'healthcare', 'research', 'study', 'treatment',
            'entertainment', 'movie', 'music', 'celebrity', 'film', 'tv'
        ]
        
        for keyword in keywords:
            if keyword in headline_lower or keyword in content_lower:
                tags.add(keyword)
                if len(tags) >= 8:
                    break
        
        return json.dumps(list(tags)[:8], ensure_ascii=False)
    
    def generate_publish_time(self, index: int) -> str:
        """ç”Ÿæˆå‘å¸ƒæ—¶é—´"""
        days_ago = random.randint(0, 30)
        hours_ago = random.randint(0, 23)
        minutes_ago = random.randint(0, 59)
        
        publish_time = datetime.now() - timedelta(days=days_ago, hours=hours_ago, minutes=minutes_ago)
        return publish_time.strftime('%Y-%m-%d %H:%M:%S')
    
    def count_words(self, text: str) -> int:
        """è®¡ç®—å•è¯æ•°"""
        if not text:
            return 0
        return len(text.split())
    
    def process_news_file(self, file_path: str, batch_size: int = 1000):
        """å¤„ç†æ–°é—»æ–‡ä»¶å¹¶æ‰¹é‡å¯¼å…¥æ•°æ®åº“"""
        conn = self.connect_mysql()
        if not conn:
            self.logger.error("æ— æ³•è¿æ¥æ•°æ®åº“")
            return False
        
        cursor = conn.cursor()
        
        try:
            processed_count = 0
            batch_data = []
            
            self.logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as file:
                # è·³è¿‡æ ‡é¢˜è¡Œ
                header = file.readline()
                self.logger.info(f"æ–‡ä»¶å¤´: {header.strip()}")
                
                for line_num, line in enumerate(file, 1):
                    try:
                        # è§£æTSVè¡Œ
                        parts = line.strip().split('\t')
                        
                        if len(parts) < 5:
                            continue
                        
                        news_id = parts[0].strip()
                        category = parts[1].strip()
                        topic = parts[2].strip()
                        headline = parts[3].strip()
                        content = parts[4].strip() if len(parts) > 4 else ''
                        title_entity = parts[5].strip() if len(parts) > 5 else '{}'
                        entity_content = parts[6].strip() if len(parts) > 6 else '{}'
                        
                        # æ•°æ®æ¸…ç†å’Œå¤„ç†
                        headline = self.clean_headline(headline)
                        content = self.clean_text(content)
                        
                        # ç”Ÿæˆå…¶ä»–å­—æ®µ
                        word_count = self.count_words(content)
                        publish_time = self.generate_publish_time(line_num)
                        source = random.choice(self.sources)
                        tags = self.generate_tags(headline, content, category, topic)
                        entities = self.parse_entities(title_entity, entity_content)
                        
                        # æ·»åŠ åˆ°æ‰¹æ¬¡æ•°æ®
                        batch_data.append((
                            news_id, headline, content, category, topic,
                            word_count, publish_time, source, tags, entities
                        ))
                        
                        # å½“æ‰¹æ¬¡æ•°æ®è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶æ‰§è¡Œæ’å…¥
                        if len(batch_data) >= batch_size:
                            self._insert_batch(cursor, batch_data)
                            conn.commit()
                            processed_count += len(batch_data)
                            self.logger.info(f"å·²å¤„ç† {processed_count} æ¡æ–°é—»è®°å½•")
                            batch_data = []
                        
                    except Exception as e:
                        self.logger.error(f"å¤„ç†è¡Œ {line_num} æ—¶å‡ºé”™: {e}")
                        continue
            
            # å¤„ç†å‰©ä½™çš„æ•°æ®
            if batch_data:
                self._insert_batch(cursor, batch_data)
                conn.commit()
                processed_count += len(batch_data)
            
            self.logger.info(f"æ•°æ®å¯¼å…¥å®Œæˆï¼æ€»å…±å¤„ç†äº† {processed_count} æ¡æ–°é—»è®°å½•")
            return True
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            conn.rollback()
            return False
        finally:
            cursor.close()
            conn.close()
    
    def _insert_batch(self, cursor, batch_data):
        """æ‰¹é‡æ’å…¥æ•°æ®"""
        insert_sql = """
        INSERT IGNORE INTO news_articles 
        (news_id, headline, content, category, topic, word_count, publish_time, source, tags, entities)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        cursor.executemany(insert_sql, batch_data)
    
    def verify_import(self):
        """éªŒè¯å¯¼å…¥ç»“æœ"""
        conn = self.connect_mysql()
        if not conn:
            return
        
        cursor = conn.cursor()
        
        try:
            # ç»Ÿè®¡æ€»æ•°
            cursor.execute("SELECT COUNT(*) FROM news_articles")
            total_count = cursor.fetchone()[0]
            
            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            cursor.execute("""
                SELECT category, COUNT(*) as count 
                FROM news_articles 
                GROUP BY category 
                ORDER BY count DESC
            """)
            category_stats = cursor.fetchall()
            
            # æŒ‰ä¸»é¢˜ç»Ÿè®¡ï¼ˆå‰10ä¸ªï¼‰
            cursor.execute("""
                SELECT topic, COUNT(*) as count 
                FROM news_articles 
                GROUP BY topic 
                ORDER BY count DESC 
                LIMIT 10
            """)
            topic_stats = cursor.fetchall()
            
            print(f"\nğŸ“Š å¯¼å…¥éªŒè¯ç»“æœ:")
            print(f"æ€»æ–°é—»æ•°é‡: {total_count}")
            
            print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
            for category, count in category_stats:
                print(f"  {category}: {count}")
            
            print(f"\nğŸ”¥ çƒ­é—¨ä¸»é¢˜ (å‰10):")
            for topic, count in topic_stats:
                print(f"  {topic}: {count}")
                
        except Exception as e:
            self.logger.error(f"éªŒè¯å¤±è´¥: {e}")
        finally:
            cursor.close()
            conn.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ PENSæ–°é—»æ•°æ®å¯¼å…¥å·¥å…·")
    print("="*50)
    
    # æ•°æ®åº“é…ç½®
    mysql_config = {
        'host': 'localhost',
        'user': 'analytics_user',
        'password': 'pass123',
        'database': 'news_analytics',
        'charset': 'utf8mb4'
    }
    
    # åˆ›å»ºå¯¼å…¥å™¨
    importer = PENSNewsImporter(mysql_config)
    
    # å¯¼å…¥æ•°æ®
    file_path = "data/PENS/news.tsv"
    
    print(f"ğŸ“ å‡†å¤‡å¯¼å…¥æ–‡ä»¶: {file_path}")
    print("âš ï¸  è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    success = importer.process_news_file(file_path, batch_size=500)
    
    if success:
        print("\nâœ… æ•°æ®å¯¼å…¥æˆåŠŸï¼")
        print("ğŸ” éªŒè¯å¯¼å…¥ç»“æœ...")
        importer.verify_import()
    else:
        print("\nâŒ æ•°æ®å¯¼å…¥å¤±è´¥ï¼")

if __name__ == "__main__":
    main() 