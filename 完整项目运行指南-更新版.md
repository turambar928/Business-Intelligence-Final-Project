# 新闻实时分析系统 - 完整运行指南 (更新版)

> **⚠️ 重要说明**: 本指南基于实际调试和运行经验更新，包含所有必要的修复和正确配置。

## 🎯 系统概览

这是一个完整的**新闻实时分析与推荐系统**，实现了：

- **数据流**: Kafka → Spark Streaming → AI分析引擎 → MySQL
- **AI功能**: 用户行为分析、新闻热度分析、趋势分析、情感分析、协同过滤推荐
- **实时处理**: 每10秒处理一批数据，支持大规模并发
- **数据可视化**: 完整的MySQL存储，支持实时查询和监控

## 📋 环境要求

### 推荐环境
- **操作系统**: Ubuntu 20.04+ (强烈推荐Linux)
- **内存**: 8GB+ (推荐12GB)
- **CPU**: 4核+ 
- **硬盘**: 50GB+
- **Java**: OpenJDK 11 (已验证兼容)
- **Python**: 3.8+

## 🚀 一、环境准备与安装

### 1. 基础环境设置
```bash
# 更新系统
sudo apt update && sudo apt upgrade -y

# 安装Java 11 (推荐，兼容性更好)
sudo apt install openjdk-11-jdk -y
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc

# 安装Python和基础工具
sudo apt install python3 python3-pip git wget -y
```

### 2. 安装MySQL并配置
```bash
# 安装MySQL
sudo apt install mysql-server -y

# 启动MySQL服务
sudo systemctl start mysql
sudo systemctl enable mysql

# 配置MySQL (重要: 使用更新的用户配置)
sudo mysql << EOF
-- 创建数据库
CREATE DATABASE IF NOT EXISTS news_analytics CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- 设置密码策略 (简化密码要求)
SET @@global.validate_password.policy=LOW;
SET @@global.validate_password.length=6;

-- 创建用户 (使用实际调试中验证的配置)
DROP USER IF EXISTS 'analytics_user'@'localhost';
CREATE USER 'analytics_user'@'localhost' IDENTIFIED BY 'pass123';
GRANT ALL PRIVILEGES ON news_analytics.* TO 'analytics_user'@'localhost';
FLUSH PRIVILEGES;

-- 验证用户创建成功
SELECT User, Host FROM mysql.user WHERE User='analytics_user';
EOF

# 测试连接
mysql -u analytics_user -p'pass123' news_analytics -e "SELECT 'MySQL连接成功!' as status;"
```

### 3. 安装Kafka
```bash
# 下载并安装Kafka
cd ~
wget https://downloads.apache.org/kafka/2.8.2/kafka_2.13-2.8.2.tgz
tar -xzf kafka_2.13-2.8.2.tgz
sudo mv kafka_2.13-2.8.2 /opt/kafka

# 设置环境变量
export KAFKA_HOME=/opt/kafka
echo 'export KAFKA_HOME=/opt/kafka' >> ~/.bashrc
echo 'export PATH=$PATH:$KAFKA_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

### 4. 安装Spark (通过pip - 推荐方式)
```bash
# 使用pip安装PySpark (包含完整Spark环境)
pip3 install pyspark==3.4.1

# 设置SPARK_HOME (重要: 使用pip安装的路径)
export SPARK_HOME=/home/$USER/.local/lib/python3.10/site-packages/pyspark
echo "export SPARK_HOME=/home/$USER/.local/lib/python3.10/site-packages/pyspark" >> ~/.bashrc
source ~/.bashrc

# 验证Spark安装
spark-submit --version
```

## 🎮 二、项目部署

### 1. 获取项目代码
```bash
# 进入工作目录
cd ~/Desktop
git clone [你的项目地址] Business-Intelligence-Final-Project
cd Business-Intelligence-Final-Project
```

### 2. 安装Python依赖
```bash
# 安装项目依赖
pip3 install -r requirements.txt

# 下载NLTK数据 (用于自然语言处理)
python3 -c "import nltk; nltk.download('punkt'); nltk.download('vader_lexicon')"
```

### 3. 初始化数据库
```bash
# 创建数据库表结构
mysql -u analytics_user -p'pass123' news_analytics < sql/init_database.sql

# 验证表创建成功
mysql -u analytics_user -p'pass123' news_analytics -e "SHOW TABLES;"
```

## 🔧 三、系统启动 (按序执行)

### 启动前检查
```bash
# 检查所有必要服务
sudo systemctl status mysql      # MySQL应该是active
java -version                    # 应该显示Java 11
python3 --version               # 应该显示Python 3.8+
which spark-submit              # 应该显示spark-submit路径
```

### 步骤1: 启动Zookeeper (终端窗口1)
```bash
cd /opt/kafka
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
bin/zookeeper-server-start.sh config/zookeeper.properties
```

**预期输出**: 看到"binding to port 0.0.0.0/0.0.0.0:2181"

### 步骤2: 启动Kafka (终端窗口2)
```bash
cd /opt/kafka
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
bin/kafka-server-start.sh config/server.properties
```

**预期输出**: 看到"started (kafka.server.KafkaServer)"

### 步骤3: 创建Kafka Topic (终端窗口3)
```bash
cd /opt/kafka
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# 创建topic
bin/kafka-topics.sh --create \
  --topic news_impression_logs \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# 验证topic创建成功
bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

### 步骤4: 启动主分析系统 (终端窗口4)
```bash
cd ~/Desktop/Business-Intelligence-Final-Project

# 设置环境变量 (重要!)
export SPARK_HOME=/home/$USER/.local/lib/python3.10/site-packages/pyspark
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# 启动系统
python3 main.py
```

**预期输出**: 
```
============================================================
新闻实时分析系统启动
============================================================
2025-06-11 00:XX:XX - INFO - 系统配置加载完成
2025-06-11 00:XX:XX - INFO - 新闻流分析器初始化完成
2025-06-11 00:XX:XX - INFO - 流数据处理已启动...
```

### 步骤5: 启动数据模拟器 (终端窗口5)
```bash
cd ~/Desktop/Business-Intelligence-Final-Project

# 启动数据生成器 (5条/秒，持续30分钟)
python3 utils/data_simulator.py --rate 5 --duration 30
```

**预期输出**:
```
==================================================
新闻数据模拟器
==================================================
Kafka服务器: localhost:9092
Topic: news_impression_logs
数据速率: 5 条/秒
运行时长: 30 分钟
==================================================
数据模拟器初始化成功
开始数据模拟...
已发送 100 条数据, 错误 0 条
```

## 📊 四、验证系统运行

### 1. 检查数据流入
```bash
# 检查Kafka消息
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
/opt/kafka/bin/kafka-console-consumer.sh \
  --topic news_impression_logs \
  --bootstrap-server localhost:9092 \
  --max-messages 3
```

### 2. 检查Spark处理日志
```bash
# 实时查看处理日志
tail -f logs/news_analytics.log | grep -E "(处理批次|保存到MySQL)"
```

**预期日志**:
```
2025-06-11 00:XX:XX - INFO - 处理批次 1, 数据量: 50
2025-06-11 00:XX:XX - INFO - 批次 1 分析结果已保存到MySQL数据库
2025-06-11 00:XX:XX - INFO - 处理批次 2, 数据量: 48
2025-06-11 00:XX:XX - INFO - 批次 2 分析结果已保存到MySQL数据库
```

### 3. 检查MySQL数据存储
```bash
# 快速检查所有表的数据量
mysql -u analytics_user -p'pass123' news_analytics -e "
SELECT 'user_behavior_analytics' as table_name, COUNT(*) as count FROM user_behavior_analytics
UNION ALL
SELECT 'news_popularity_analytics', COUNT(*) FROM news_popularity_analytics  
UNION ALL
SELECT 'trending_analytics', COUNT(*) FROM trending_analytics
UNION ALL
SELECT 'recommendation_results', COUNT(*) FROM recommendation_results;"

# 查看最新的分析结果
mysql -u analytics_user -p'pass123' news_analytics -e "
SELECT batch_id, timestamp, avg_click_rate, active_users, total_impressions 
FROM user_behavior_analytics 
ORDER BY id DESC LIMIT 5;"
```

### 4. 实时监控数据增长
```bash
# 监控数据库记录数量变化 (每5秒刷新)
watch -n 5 'mysql -u analytics_user -p"pass123" news_analytics -e "SELECT COUNT(*) as total_records FROM user_behavior_analytics;"'
```

## 📈 五、AI分析功能详解

### 1. 用户行为分析
- **平均点击率**: 实时计算CTR趋势
- **活跃用户数**: 统计每批次活跃用户
- **曝光-点击统计**: 完整的漏斗分析

### 2. 新闻热度分析  
- **热门新闻排行**: 基于点击数和CTR
- **热度得分计算**: 综合多维度指标
- **实时更新**: 每10秒更新热门榜单

### 3. 趋势分析
- **上升趋势新闻**: 识别快速上升的内容
- **趋势得分**: 基于点击增长率计算
- **分类趋势**: 不同类别的内容趋势

### 4. 协同过滤推荐
- **用户相似度**: Jaccard相似度计算
- **个性化推荐**: 基于相似用户的推荐
- **实时更新**: 用户行为变化时实时调整

### 5. 情感分析 (扩展功能)
- **情感倾向**: 正面/负面/中性分类
- **置信度评估**: 分析结果可信度
- **情感趋势**: 内容情感倾向变化

## 🔍 六、系统监控与查询

### 实时监控命令
```bash
# 1. 监控系统状态
ps aux | grep -E "(python3 main.py|kafka|zookeeper)"

# 2. 监控Kafka消息流
/opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic news_impression_logs

# 3. 监控处理日志
tail -f logs/news_analytics.log

# 4. 监控数据库写入
watch -n 10 'mysql -u analytics_user -p"pass123" news_analytics -e "
SELECT MAX(batch_id) as latest_batch, 
       COUNT(*) as total_records,
       MAX(timestamp) as latest_time 
FROM user_behavior_analytics;"'
```

### 数据分析查询示例
```sql
-- 查看最热门的新闻
SELECT news_id, clicks, popularity_score 
FROM news_popularity_analytics 
ORDER BY popularity_score DESC LIMIT 10;

-- 查看用户行为趋势
SELECT 
    DATE_FORMAT(timestamp, '%H:%i') as time_slot,
    AVG(avg_click_rate) as avg_ctr,
    SUM(total_impressions) as total_impressions
FROM user_behavior_analytics 
WHERE timestamp >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY DATE_FORMAT(timestamp, '%H:%i')
ORDER BY time_slot;

-- 查看推荐效果
SELECT 
    user_id, 
    COUNT(*) as recommendation_count,
    AVG(rank_score) as avg_score
FROM recommendation_results
GROUP BY user_id
ORDER BY recommendation_count DESC
LIMIT 10;
```

## 🛠️ 七、故障排除指南

### 常见问题与解决方案

#### Q1: MySQL连接被拒绝
```bash
# 检查MySQL服务状态
sudo systemctl status mysql

# 重新创建用户
sudo mysql -e "
DROP USER IF EXISTS 'analytics_user'@'localhost';
CREATE USER 'analytics_user'@'localhost' IDENTIFIED BY 'pass123';
GRANT ALL PRIVILEGES ON news_analytics.* TO 'analytics_user'@'localhost';
FLUSH PRIVILEGES;"

# 测试连接
mysql -u analytics_user -p'pass123' news_analytics -e "SELECT 'OK' as status;"
```

#### Q2: Spark启动失败
```bash
# 检查SPARK_HOME设置
echo $SPARK_HOME
# 应该输出: /home/用户名/.local/lib/python3.10/site-packages/pyspark

# 检查Java版本
java -version
# 应该显示Java 11

# 重新设置环境变量
export SPARK_HOME=/home/$USER/.local/lib/python3.10/site-packages/pyspark
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

#### Q3: Kafka连接问题
```bash
# 检查Kafka服务状态
netstat -tulpn | grep :9092

# 检查Zookeeper状态  
netstat -tulpn | grep :2181

# 重启Kafka (如果需要)
cd /opt/kafka
bin/kafka-server-stop.sh
bin/kafka-server-start.sh config/server.properties
```

#### Q4: 数据未写入数据库
```bash
# 检查日志中的错误信息
grep -E "(ERROR|WARN)" logs/news_analytics.log | tail -10

# 检查MySQL表权限
mysql -u analytics_user -p'pass123' news_analytics -e "SHOW GRANTS;"

# 手动测试数据插入
mysql -u analytics_user -p'pass123' news_analytics -e "
INSERT INTO user_behavior_analytics 
(batch_id, timestamp, avg_click_rate, active_users, total_impressions, total_clicks)
VALUES (9999, NOW(), 0.15, 100, 500, 75);"
```

## 🎯 八、性能优化建议

### 1. 系统配置优化
```bash
# 增加系统文件限制
echo '* soft nofile 65536' | sudo tee -a /etc/security/limits.conf
echo '* hard nofile 65536' | sudo tee -a /etc/security/limits.conf

# 优化虚拟内存
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
```

### 2. Spark配置优化
```python
# 在main.py中调整Spark配置
'spark': {
    'app_name': 'NewsRealTimeAnalytics',
    'batch_interval': 10,  # 可调整为5或15秒
    'checkpoint_dir': './checkpoints',
    'master': 'local[*]'   # 使用所有可用CPU核心
}
```

### 3. MySQL配置优化
```bash
# 编辑MySQL配置文件
sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf

# 添加以下配置
[mysqld]
innodb_buffer_pool_size = 2G
innodb_log_file_size = 256M
max_connections = 1000
query_cache_size = 128M
```

## 🚀 九、快速启动脚本

创建一个一键启动脚本 `quick_start.sh`:

```bash
#!/bin/bash
echo "=== 新闻实时分析系统 - 快速启动 ==="

# 设置环境变量
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export SPARK_HOME=/home/$USER/.local/lib/python3.10/site-packages/pyspark
export KAFKA_HOME=/opt/kafka

# 检查服务状态
echo "检查MySQL状态..."
systemctl is-active mysql || (echo "MySQL未运行，请先启动MySQL" && exit 1)

echo "检查Kafka状态..."
netstat -tulpn | grep :9092 || (echo "Kafka未运行，请先启动Kafka" && exit 1)

echo "启动主分析系统..."
python3 main.py &

echo "等待5秒后启动数据模拟器..."
sleep 5
python3 utils/data_simulator.py --rate 5 --duration 60 &

echo "系统启动完成！"
echo "查看日志: tail -f logs/news_analytics.log"
echo "查看数据: mysql -u analytics_user -p'pass123' news_analytics"
```

## 📋 十、成功运行标志

当系统正确运行时，你应该看到：

1. **日志输出**: 每10秒一条"处理批次 X, 数据量: Y"
2. **MySQL数据**: 5个分析表中都有不断增长的记录
3. **进程状态**: Zookeeper、Kafka、main.py都在运行
4. **无错误日志**: 没有连接失败或保存失败的错误

### 验证命令
```bash
# 一键检查系统状态
python3 how_to_check_mysql_storage.py
```

---

**最后更新**: 2025-06-11  
**验证状态**: ✅ 已在Ubuntu 20.04上完整测试通过  
**数据库配置**: analytics_user / pass123  
**实时状态**: 🟢 系统正常运行，AI分析数据实时流入MySQL 