#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°é—»å®æ—¶åˆ†æç³»ç»Ÿä¸»å¯åŠ¨æ–‡ä»¶ - å¢å¼ºç‰ˆ
æ•´åˆKafkaæ¶ˆè´¹ã€Spark Streamingå¤„ç†ã€AIåˆ†æå’ŒMySQLå­˜å‚¨
ä½¿ç”¨å¢å¼ºçš„åˆ†æç»„ä»¶ï¼Œæ”¯æŒæ–°çš„æ—¥å¿—æ ¼å¼å’Œä¸ƒå¤§åˆ†æåŠŸèƒ½
"""

import sys
import os
import logging
import signal
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å¢å¼ºçš„è‡ªå®šä¹‰æ¨¡å—
from src.enhanced_spark_streaming_analyzer import EnhancedNewsStreamingAnalyzer
from src.enhanced_ai_analysis_engine import EnhancedAIAnalysisEngine
from src.enhanced_data_generator import EnhancedDataGenerator

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler('logs/enhanced_news_analytics.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = ['logs', 'checkpoints', 'data', 'models', 'web_logs']
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"åˆ›å»ºç›®å½•: {directory}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("æ–°é—»å®æ—¶åˆ†æç³»ç»Ÿå¯åŠ¨ - å¢å¼ºç‰ˆ")
    print("="*60)
    
    # åˆ›å»ºç›®å½•
    create_directories()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # å¢å¼ºç‰ˆé…ç½®ä¿¡æ¯
    config = {
        'kafka': {
            'bootstrap_servers': 'localhost:9092',
            'topic': 'news_impression_logs',
            'consumer_group': 'enhanced_news_analytics_group'
        },
        'mysql': {
            'host': 'localhost',
            'port': 3306,
            'database': 'news_analytics',
            'user': 'analytics_user',
            'password': 'pass123',
            'charset': 'utf8mb4'
        },
        'spark': {
            'app_name': 'EnhancedNewsRealTimeAnalytics',
            'batch_interval': 10,
            'checkpoint_dir': './checkpoints',
            'master': 'local[*]'
        },
        'analysis': {
            'enable_ai_analysis': True,
            'enable_viral_prediction': True,
            'enable_real_time_recommendations': True,
            'enable_sentiment_analysis': True,
            'save_analysis_results': True
        }
    }
    
    logger.info("å¢å¼ºç‰ˆç³»ç»Ÿé…ç½®åŠ è½½å®Œæˆ")
    
    try:
        # åˆå§‹åŒ–å¢å¼ºçš„åˆ†æå™¨
        logger.info("åˆå§‹åŒ–å¢å¼ºçš„Sparkæµå¤„ç†åˆ†æå™¨...")
        analyzer = EnhancedNewsStreamingAnalyzer(config['kafka'], config['mysql'])
        
        # åˆå§‹åŒ–AIåˆ†æå¼•æ“ï¼ˆå¯é€‰ç‹¬ç«‹æµ‹è¯•ï¼‰
        logger.info("åˆå§‹åŒ–å¢å¼ºçš„AIåˆ†æå¼•æ“...")
        ai_engine = EnhancedAIAnalysisEngine(config['mysql'])
        
        # åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
        logger.info("åˆå§‹åŒ–å¢å¼ºçš„æ•°æ®ç”Ÿæˆå™¨...")
        data_generator = EnhancedDataGenerator("data/")
        
        print("\nğŸš€ å¢å¼ºç‰ˆç³»ç»Ÿç»„ä»¶:")
        print("1. âœ… Kafkaæ¶ˆè´¹è€… - æ¥æ”¶å¢å¼ºæ ¼å¼æ–°é—»æ—¥å¿—")
        print("2. âœ… Spark Streaming - å®æ—¶æ•°æ®å¤„ç†å’Œåˆ†æ")
        print("3. âœ… AIåˆ†æå¼•æ“ - ä¸ƒå¤§æ™ºèƒ½åˆ†æåŠŸèƒ½")
        print("   - æ–°é—»ç”Ÿå‘½å‘¨æœŸåˆ†æ")
        print("   - åˆ†ç±»è¶‹åŠ¿åˆ†æ")
        print("   - ç”¨æˆ·å…´è¶£è·Ÿè¸ª")
        print("   - çˆ†æ¬¾æ–°é—»é¢„æµ‹")
        print("   - å®æ—¶æ¨èç³»ç»Ÿ")
        print("   - æƒ…æ„Ÿåˆ†æ")
        print("   - å¤šç»´åº¦æŸ¥è¯¢ç»Ÿè®¡")
        print("4. âœ… MySQLå­˜å‚¨ - å¢å¼ºçš„åˆ†æç»“æœæŒä¹…åŒ–")
        print("5. âœ… æ•°æ®ç”Ÿæˆå™¨ - æ”¯æŒæ–°æ—¥å¿—æ ¼å¼çš„æµ‹è¯•æ•°æ®")
        
        print(f"\nğŸ“Š æ—¥å¿—æ ¼å¼: å¢å¼ºç‰ˆJSONæ ¼å¼ (ç¬¦åˆdocs/new_log_format.mdè§„èŒƒ)")
        print(f"ğŸ“ æ—¥å¿—ç›®å½•: web_logs/")
        print(f"ğŸ”„ å¤„ç†é—´éš”: {config['spark']['batch_interval']}ç§’")
        print(f"ğŸ¯ Kafkaä¸»é¢˜: {config['kafka']['topic']}")
        
        # å¯åŠ¨æµå¤„ç†åˆ†æ
        logger.info("å¯åŠ¨å¢å¼ºç‰ˆæµå¤„ç†åˆ†æ...")
        print("\nğŸ¯ ç³»ç»Ÿæ­£åœ¨è¿è¡Œä¸­...")
        print("   - å®æ—¶æ¥æ”¶Kafkaæ—¥å¿—æµ")
        print("   - æ‰§è¡Œä¸ƒå¤§AIåˆ†æåŠŸèƒ½")
        print("   - å­˜å‚¨åˆ†æç»“æœåˆ°MySQL")
        print("   - æŒ‰Ctrl+Cå®‰å…¨åœæ­¢")
        
        analyzer.start_streaming()
        
        # ä¿æŒè¿è¡Œ
        while True:
            time.sleep(30)
            logger.info(f"å¢å¼ºç‰ˆç³»ç»Ÿè¿è¡Œæ­£å¸¸ - {datetime.now()}")
            
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­å¢å¼ºç‰ˆç³»ç»Ÿ...")
        print("\nğŸ›‘ ç³»ç»Ÿæ­£åœ¨å®‰å…¨å…³é—­...")
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆç³»ç»Ÿè¿è¡Œé”™è¯¯: {str(e)}")
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import traceback
        traceback.print_exc()
    finally:
        print("âœ… å¢å¼ºç‰ˆç³»ç»Ÿå·²åœæ­¢")

if __name__ == "__main__":
    main()
