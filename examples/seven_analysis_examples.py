#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸ƒå¤§åˆ†æåŠŸèƒ½ç¤ºä¾‹è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å¢å¼ºçš„æ–°é—»åˆ†æç³»ç»Ÿ
"""

import mysql.connector
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import json

plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False

class SevenAnalysisExamples:
    """ä¸ƒå¤§åˆ†æåŠŸèƒ½ç¤ºä¾‹"""
    
    def __init__(self, mysql_config):
        self.mysql_config = mysql_config
        
    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        return mysql.connector.connect(**self.mysql_config)
    
    def analysis_1_news_lifecycle(self, news_id: str = None):
        """åˆ†æ1ï¼šå•ä¸ªæ–°é—»çš„ç”Ÿå‘½å‘¨æœŸæŸ¥è¯¢"""
        print("ğŸ” åˆ†æ1ï¼šæ–°é—»ç”Ÿå‘½å‘¨æœŸåˆ†æ")
        print("="*50)
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        if not news_id:
            # è·å–æœ€è¿‘çš„çƒ­é—¨æ–°é—»
            cursor.execute("""
                SELECT news_id FROM news_lifecycle 
                WHERE calculated_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                ORDER BY popularity_score DESC LIMIT 1
            """)
            result = cursor.fetchone()
            news_id = result[0] if result else "N12345"
        
        # æŸ¥è¯¢æ–°é—»åœ¨ä¸åŒæ—¶é—´æ®µçš„æµè¡Œå˜åŒ–
        query = """
        SELECT 
            time_period,
            total_reads,
            total_shares,
            total_likes,
            unique_readers,
            avg_dwell_time,
            popularity_score,
            growth_rate,
            calculated_at
        FROM news_lifecycle 
        WHERE news_id = %s
        ORDER BY 
            FIELD(time_period, '1h', '6h', '12h', '1d', '3d', '7d')
        """
        
        df = pd.read_sql(query, conn, params=[news_id])
        
        if not df.empty:
            print(f"ğŸ“° æ–°é—»ID: {news_id}")
            print(f"ğŸ“Š ç”Ÿå‘½å‘¨æœŸæ•°æ®:")
            print(df.to_string(index=False))
            
            # å¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'æ–°é—» {news_id} ç”Ÿå‘½å‘¨æœŸåˆ†æ', fontsize=16)
            
            # é˜…è¯»é‡å˜åŒ–
            axes[0,0].plot(df['time_period'], df['total_reads'], marker='o')
            axes[0,0].set_title('é˜…è¯»é‡å˜åŒ–')
            axes[0,0].set_xlabel('æ—¶é—´æ®µ')
            axes[0,0].set_ylabel('é˜…è¯»é‡')
            
            # æµè¡Œåº¦åˆ†æ•°
            axes[0,1].bar(df['time_period'], df['popularity_score'])
            axes[0,1].set_title('æµè¡Œåº¦åˆ†æ•°')
            axes[0,1].set_xlabel('æ—¶é—´æ®µ')
            axes[0,1].set_ylabel('æµè¡Œåº¦åˆ†æ•°')
            
            # äº’åŠ¨æ•°æ®
            axes[1,0].plot(df['time_period'], df['total_shares'], marker='s', label='åˆ†äº«')
            axes[1,0].plot(df['time_period'], df['total_likes'], marker='^', label='ç‚¹èµ')
            axes[1,0].set_title('äº’åŠ¨æ•°æ®')
            axes[1,0].set_xlabel('æ—¶é—´æ®µ')
            axes[1,0].set_ylabel('æ•°é‡')
            axes[1,0].legend()
            
            # å¹³å‡åœç•™æ—¶é—´
            axes[1,1].plot(df['time_period'], df['avg_dwell_time'], marker='d', color='red')
            axes[1,1].set_title('å¹³å‡åœç•™æ—¶é—´')
            axes[1,1].set_xlabel('æ—¶é—´æ®µ')
            axes[1,1].set_ylabel('åœç•™æ—¶é—´(ç§’)')
            
            plt.tight_layout()
            plt.show()
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ–°é—» {news_id} çš„ç”Ÿå‘½å‘¨æœŸæ•°æ®")
        
        conn.close()
    
    def analysis_2_category_trends(self, category: str = None, hours: int = 24):
        """åˆ†æ2ï¼šä¸åŒç±»åˆ«æ–°é—»å˜åŒ–æƒ…å†µç»Ÿè®¡"""
        print("ğŸ” åˆ†æ2ï¼šæ–°é—»åˆ†ç±»è¶‹åŠ¿åˆ†æ")
        print("="*50)
        
        conn = self.connect_db()
        
        # æŸ¥è¯¢åˆ†ç±»è¶‹åŠ¿æ•°æ®
        query = """
        SELECT 
            category,
            topic,
            DATE_FORMAT(date_hour, '%%Y-%%m-%%d %%H:00') as hour_period,
            total_articles,
            total_reads,
            total_interactions,
            avg_engagement,
            trending_keywords,
            hot_topics
        FROM category_trends 
        WHERE date_hour >= DATE_SUB(NOW(), INTERVAL %s HOUR)
        """ + (f" AND category = '{category}'" if category else "") + """
        ORDER BY date_hour DESC, avg_engagement DESC
        """
        
        df = pd.read_sql(query, conn, params=[hours])
        
        if not df.empty:
            print(f"ğŸ“Š è¿‡å»{hours}å°æ—¶çš„åˆ†ç±»è¶‹åŠ¿æ•°æ®:")
            
            # æŒ‰åˆ†ç±»æ±‡æ€»
            category_summary = df.groupby('category').agg({
                'total_articles': 'sum',
                'total_reads': 'sum',
                'total_interactions': 'sum',
                'avg_engagement': 'mean'
            }).round(2)
            
            print("\nğŸ“ˆ åˆ†ç±»æ±‡æ€»:")
            print(category_summary)
            
            # æ‰¾å‡ºæœ€çƒ­é—¨çš„è¯é¢˜
            top_topics = df.nlargest(10, 'avg_engagement')[['category', 'topic', 'avg_engagement', 'hour_period']]
            print("\nğŸ”¥ æœ€çƒ­é—¨è¯é¢˜ (TOP 10):")
            print(top_topics.to_string(index=False))
            
            # å¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'è¿‡å»{hours}å°æ—¶æ–°é—»åˆ†ç±»è¶‹åŠ¿åˆ†æ', fontsize=16)
            
            # å„åˆ†ç±»æ–‡ç« æ•°é‡
            category_counts = df.groupby('category')['total_articles'].sum()
            axes[0,0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')
            axes[0,0].set_title('å„åˆ†ç±»æ–‡ç« å æ¯”')
            
            # å„åˆ†ç±»å¹³å‡å‚ä¸åº¦
            category_engagement = df.groupby('category')['avg_engagement'].mean()
            axes[0,1].bar(category_engagement.index, category_engagement.values)
            axes[0,1].set_title('å„åˆ†ç±»å¹³å‡å‚ä¸åº¦')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # æ—¶é—´è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
            if len(df['hour_period'].unique()) > 1:
                hourly_data = df.groupby('hour_period')['total_interactions'].sum()
                axes[1,0].plot(range(len(hourly_data)), hourly_data.values, marker='o')
                axes[1,0].set_title('å°æ—¶äº’åŠ¨è¶‹åŠ¿')
                axes[1,0].set_xlabel('æ—¶é—´ç‚¹')
                axes[1,0].set_ylabel('æ€»äº’åŠ¨æ•°')
            
            # é˜…è¯»é‡ vs å‚ä¸åº¦æ•£ç‚¹å›¾
            axes[1,1].scatter(df['total_reads'], df['avg_engagement'], alpha=0.6)
            axes[1,1].set_title('é˜…è¯»é‡ vs å‚ä¸åº¦')
            axes[1,1].set_xlabel('æ€»é˜…è¯»é‡')
            axes[1,1].set_ylabel('å¹³å‡å‚ä¸åº¦')
            
            plt.tight_layout()
            plt.show()
        else:
            print(f"âŒ æœªæ‰¾åˆ°è¿‡å»{hours}å°æ—¶çš„åˆ†ç±»è¶‹åŠ¿æ•°æ®")
        
        conn.close()
    
    def analysis_3_user_interests(self, user_id: str = None, days: int = 30):
        """åˆ†æ3ï¼šç”¨æˆ·å…´è¶£å˜åŒ–ç»Ÿè®¡æŸ¥è¯¢"""
        print("ğŸ” åˆ†æ3ï¼šç”¨æˆ·å…´è¶£å˜åŒ–åˆ†æ")
        print("="*50)
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        if not user_id:
            # è·å–ä¸€ä¸ªæ´»è·ƒç”¨æˆ·
            cursor.execute("""
                SELECT user_id FROM user_interest_evolution 
                ORDER BY calculated_at DESC LIMIT 1
            """)
            result = cursor.fetchone()
            user_id = result[0] if result else "U335175"
        
        # æŸ¥è¯¢ç”¨æˆ·å…´è¶£å˜åŒ–
        query = """
        SELECT 
            date_period,
            interest_categories,
            interest_scores,
            reading_patterns,
            behavior_changes,
            engagement_trend,
            diversity_score
        FROM user_interest_evolution 
        WHERE user_id = %s 
        AND date_period >= DATE_SUB(CURDATE(), INTERVAL %s DAY)
        ORDER BY date_period
        """
        
        df = pd.read_sql(query, conn, params=[user_id, days])
        
        if not df.empty:
            print(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id}")
            print(f"ğŸ“Š è¿‡å»{days}å¤©çš„å…´è¶£å˜åŒ–æ•°æ®:")
            
            # è§£æJSONæ•°æ®
            df['interest_categories'] = df['interest_categories'].apply(
                lambda x: json.loads(x) if isinstance(x, str) else x
            )
            df['interest_scores'] = df['interest_scores'].apply(
                lambda x: json.loads(x) if isinstance(x, str) else x
            )
            
            print(f"\nğŸ“ˆ å…´è¶£æ¼”åŒ–è¶‹åŠ¿:")
            print(df[['date_period', 'engagement_trend', 'diversity_score']].to_string(index=False))
            
            # åˆ†æå…´è¶£å˜åŒ–
            if len(df) > 1:
                latest_interests = df.iloc[-1]['interest_categories']
                earliest_interests = df.iloc[0]['interest_categories']
                
                print(f"\nğŸ”„ å…´è¶£å˜åŒ–å¯¹æ¯”:")
                print(f"æ—©æœŸå…´è¶£: {earliest_interests}")
                print(f"æœ€æ–°å…´è¶£: {latest_interests}")
                
                # è®¡ç®—å…´è¶£ç¨³å®šæ€§
                common_interests = set(latest_interests) & set(earliest_interests)
                stability_score = len(common_interests) / len(set(latest_interests) | set(earliest_interests))
                print(f"å…´è¶£ç¨³å®šæ€§: {stability_score:.2f}")
            
            # å¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'ç”¨æˆ· {user_id} å…´è¶£å˜åŒ–åˆ†æ', fontsize=16)
            
            # å‚ä¸åº¦è¶‹åŠ¿
            axes[0,0].plot(df['date_period'], df['engagement_trend'], marker='o')
            axes[0,0].set_title('å‚ä¸åº¦è¶‹åŠ¿')
            axes[0,0].set_xlabel('æ—¥æœŸ')
            axes[0,0].set_ylabel('å‚ä¸åº¦')
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # å…´è¶£å¤šæ ·æ€§
            axes[0,1].plot(df['date_period'], df['diversity_score'], marker='s', color='green')
            axes[0,1].set_title('å…´è¶£å¤šæ ·æ€§å˜åŒ–')
            axes[0,1].set_xlabel('æ—¥æœŸ')
            axes[0,1].set_ylabel('å¤šæ ·æ€§åˆ†æ•°')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # å…´è¶£åˆ†å¸ƒï¼ˆæœ€è¿‘ä¸€æœŸï¼‰
            if df.iloc[-1]['interest_scores']:
                latest_scores = df.iloc[-1]['interest_scores']
                if isinstance(latest_scores, dict):
                    axes[1,0].bar(latest_scores.keys(), latest_scores.values())
                    axes[1,0].set_title('å½“å‰å…´è¶£åˆ†å¸ƒ')
                    axes[1,0].set_xlabel('å…´è¶£ç±»åˆ«')
                    axes[1,0].set_ylabel('å…´è¶£åˆ†æ•°')
                    axes[1,0].tick_params(axis='x', rotation=45)
            
            # å‚ä¸åº¦ vs å¤šæ ·æ€§
            axes[1,1].scatter(df['engagement_trend'], df['diversity_score'], alpha=0.7)
            axes[1,1].set_title('å‚ä¸åº¦ vs å…´è¶£å¤šæ ·æ€§')
            axes[1,1].set_xlabel('å‚ä¸åº¦è¶‹åŠ¿')
            axes[1,1].set_ylabel('å¤šæ ·æ€§åˆ†æ•°')
            
            plt.tight_layout()
            plt.show()
        else:
            print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ· {user_id} çš„å…´è¶£å˜åŒ–æ•°æ®")
        
        conn.close()
    
    def analysis_4_multidim_statistics(self, conditions: dict = None):
        """åˆ†æ4ï¼šå¤šç»´åº¦ç»Ÿè®¡æŸ¥è¯¢"""
        print("ğŸ” åˆ†æ4ï¼šå¤šç»´åº¦ç»Ÿè®¡æŸ¥è¯¢")
        print("="*50)
        
        conn = self.connect_db()
        
        # é»˜è®¤æŸ¥è¯¢æ¡ä»¶
        if not conditions:
            conditions = {
                'time_range': '24h',
                'category': None,
                'min_word_count': 100,
                'device_type': None
            }
        
        # æ„å»ºåŠ¨æ€æŸ¥è¯¢
        base_query = """
        SELECT 
            ne.category,
            ne.topic,
            ne.word_count,
            ue.device_type,
            ue.reading_time_of_day,
            ue.is_weekend,
            COUNT(*) as interaction_count,
            COUNT(DISTINCT ue.user_id) as unique_users,
            AVG(ue.dwell_time) as avg_dwell_time,
            AVG(ue.engagement_score) as avg_engagement,
            SUM(CASE WHEN ue.action_type = 'share' THEN 1 ELSE 0 END) as total_shares,
            SUM(CASE WHEN ue.action_type = 'like' THEN 1 ELSE 0 END) as total_likes
        FROM user_events ue
        JOIN news_articles ne ON ue.news_id = ne.news_id
        WHERE ue.timestamp >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
        """
        
        params = []
        
        # æ·»åŠ æŸ¥è¯¢æ¡ä»¶
        if conditions.get('category'):
            base_query += " AND ne.category = %s"
            params.append(conditions['category'])
        
        if conditions.get('min_word_count'):
            base_query += " AND ne.word_count >= %s"
            params.append(conditions['min_word_count'])
        
        if conditions.get('device_type'):
            base_query += " AND ue.device_type = %s"
            params.append(conditions['device_type'])
        
        base_query += """
        GROUP BY ne.category, ne.topic, ne.word_count, ue.device_type, 
                 ue.reading_time_of_day, ue.is_weekend
        ORDER BY avg_engagement DESC
        LIMIT 50
        """
        
        df = pd.read_sql(base_query, conn, params=params)
        
        if not df.empty:
            print(f"ğŸ“Š å¤šç»´åº¦ç»Ÿè®¡ç»“æœ (æŸ¥è¯¢æ¡ä»¶: {conditions}):")
            print(f"ğŸ“ å…±æ‰¾åˆ° {len(df)} æ¡è®°å½•")
            
            # åŸºç¡€ç»Ÿè®¡
            print(f"\nğŸ“ˆ åŸºç¡€ç»Ÿè®¡:")
            print(f"æ€»äº’åŠ¨æ•°: {df['interaction_count'].sum():,}")
            print(f"ç‹¬ç«‹ç”¨æˆ·æ•°: {df['unique_users'].sum():,}")
            print(f"å¹³å‡åœç•™æ—¶é—´: {df['avg_dwell_time'].mean():.1f}ç§’")
            print(f"å¹³å‡å‚ä¸åº¦: {df['avg_engagement'].mean():.3f}")
            
            # æŒ‰ä¸åŒç»´åº¦ç»Ÿè®¡
            print(f"\nğŸ“Š æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            category_stats = df.groupby('category').agg({
                'interaction_count': 'sum',
                'unique_users': 'sum',
                'avg_engagement': 'mean'
            }).round(2)
            print(category_stats)
            
            print(f"\nğŸ“± æŒ‰è®¾å¤‡ç±»å‹ç»Ÿè®¡:")
            device_stats = df.groupby('device_type').agg({
                'interaction_count': 'sum',
                'avg_dwell_time': 'mean',
                'avg_engagement': 'mean'
            }).round(2)
            print(device_stats)
            
            print(f"\nâ° æŒ‰é˜…è¯»æ—¶é—´æ®µç»Ÿè®¡:")
            time_stats = df.groupby('reading_time_of_day').agg({
                'interaction_count': 'sum',
                'avg_engagement': 'mean'
            }).round(2)
            print(time_stats)
            
            # å¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('å¤šç»´åº¦ç»Ÿè®¡åˆ†æ', fontsize=16)
            
            # ç±»åˆ«å‚ä¸åº¦
            category_engagement = df.groupby('category')['avg_engagement'].mean()
            axes[0,0].bar(category_engagement.index, category_engagement.values)
            axes[0,0].set_title('å„ç±»åˆ«å¹³å‡å‚ä¸åº¦')
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # è®¾å¤‡ç±»å‹åˆ†å¸ƒ
            device_counts = df.groupby('device_type')['interaction_count'].sum()
            axes[0,1].pie(device_counts.values, labels=device_counts.index, autopct='%1.1f%%')
            axes[0,1].set_title('è®¾å¤‡ç±»å‹äº’åŠ¨åˆ†å¸ƒ')
            
            # é˜…è¯»æ—¶é—´æ®µåˆ†æ
            time_engagement = df.groupby('reading_time_of_day')['avg_engagement'].mean()
            axes[1,0].bar(time_engagement.index, time_engagement.values, color='orange')
            axes[1,0].set_title('ä¸åŒæ—¶é—´æ®µå‚ä¸åº¦')
            
            # å­—æ•° vs å‚ä¸åº¦
            axes[1,1].scatter(df['word_count'], df['avg_engagement'], alpha=0.6)
            axes[1,1].set_title('æ–‡ç« å­—æ•° vs å‚ä¸åº¦')
            axes[1,1].set_xlabel('å­—æ•°')
            axes[1,1].set_ylabel('å¹³å‡å‚ä¸åº¦')
            
            plt.tight_layout()
            plt.show()
        else:
            print(f"âŒ æ ¹æ®æ¡ä»¶ {conditions} æœªæ‰¾åˆ°åŒ¹é…æ•°æ®")
        
        conn.close()
    
    def analysis_5_viral_prediction(self, threshold: float = 0.7):
        """åˆ†æ5ï¼šçˆ†æ¬¾æ–°é—»é¢„æµ‹åˆ†æ"""
        print("ğŸ” åˆ†æ5ï¼šçˆ†æ¬¾æ–°é—»é¢„æµ‹åˆ†æ")
        print("="*50)
        
        conn = self.connect_db()
        
        # æŸ¥è¯¢ç—…æ¯’æ€§é¢„æµ‹æ•°æ®
        query = """
        SELECT 
            vp.news_id,
            na.headline,
            na.category,
            na.topic,
            vp.viral_score,
            vp.predicted_peak_time,
            vp.key_factors,
            vp.social_signals,
            vp.prediction_accuracy,
            vp.created_at
        FROM viral_news_prediction vp
        JOIN news_articles na ON vp.news_id = na.news_id
        WHERE vp.created_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
        ORDER BY vp.viral_score DESC
        LIMIT 20
        """
        
        df = pd.read_sql(query, conn)
        
        if not df.empty:
            print(f"ğŸ”¥ æœ€æ–°çˆ†æ¬¾é¢„æµ‹ç»“æœ:")
            print(f"ğŸ“ å…±åˆ†æ {len(df)} ç¯‡æ–°é—»")
            
            # é«˜ç—…æ¯’æ€§æ–°é—»
            high_viral = df[df['viral_score'] >= threshold]
            print(f"\nâ­ é«˜ç—…æ¯’æ€§æ–°é—» (ç—…æ¯’æ€§åˆ†æ•° >= {threshold}):")
            print(f"ğŸ“Š å…± {len(high_viral)} ç¯‡")
            
            if not high_viral.empty:
                for _, row in high_viral.head(5).iterrows():
                    print(f"\nğŸ† æ–°é—»ID: {row['news_id']}")
                    print(f"ğŸ“° æ ‡é¢˜: {row['headline'][:50]}...")
                    print(f"ğŸ“Š ç—…æ¯’æ€§åˆ†æ•°: {row['viral_score']:.3f}")
                    print(f"ğŸ“ˆ é¢„æµ‹å³°å€¼æ—¶é—´: {row['predicted_peak_time']}")
                    
                    # è§£æå…³é”®å› ç´ 
                    if row['key_factors']:
                        factors = json.loads(row['key_factors']) if isinstance(row['key_factors'], str) else row['key_factors']
                        print(f"ğŸ”‘ å…³é”®å› ç´ : {factors}")
            
            # ç»Ÿè®¡åˆ†æ
            print(f"\nğŸ“ˆ ç—…æ¯’æ€§ç»Ÿè®¡:")
            print(f"å¹³å‡ç—…æ¯’æ€§åˆ†æ•°: {df['viral_score'].mean():.3f}")
            print(f"æœ€é«˜ç—…æ¯’æ€§åˆ†æ•°: {df['viral_score'].max():.3f}")
            print(f"ç—…æ¯’æ€§åˆ†æ•°æ ‡å‡†å·®: {df['viral_score'].std():.3f}")
            
            # æŒ‰ç±»åˆ«åˆ†æ
            category_viral = df.groupby('category')['viral_score'].agg(['mean', 'max', 'count']).round(3)
            print(f"\nğŸ“Š æŒ‰ç±»åˆ«ç—…æ¯’æ€§åˆ†æ:")
            print(category_viral)
            
            # å¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('çˆ†æ¬¾æ–°é—»é¢„æµ‹åˆ†æ', fontsize=16)
            
            # ç—…æ¯’æ€§åˆ†æ•°åˆ†å¸ƒ
            axes[0,0].hist(df['viral_score'], bins=20, alpha=0.7, color='red')
            axes[0,0].axvline(threshold, color='black', linestyle='--', label=f'é˜ˆå€¼ {threshold}')
            axes[0,0].set_title('ç—…æ¯’æ€§åˆ†æ•°åˆ†å¸ƒ')
            axes[0,0].set_xlabel('ç—…æ¯’æ€§åˆ†æ•°')
            axes[0,0].set_ylabel('é¢‘æ•°')
            axes[0,0].legend()
            
            # å„ç±»åˆ«ç—…æ¯’æ€§å¯¹æ¯”
            category_means = df.groupby('category')['viral_score'].mean()
            axes[0,1].bar(category_means.index, category_means.values, color='orange')
            axes[0,1].set_title('å„ç±»åˆ«å¹³å‡ç—…æ¯’æ€§')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # æ—¶é—´è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰åˆ›å»ºæ—¶é—´ï¼‰
            df['hour'] = pd.to_datetime(df['created_at']).dt.hour
            hourly_viral = df.groupby('hour')['viral_score'].mean()
            axes[1,0].plot(hourly_viral.index, hourly_viral.values, marker='o')
            axes[1,0].set_title('ä¸åŒæ—¶é—´ç—…æ¯’æ€§è¶‹åŠ¿')
            axes[1,0].set_xlabel('å°æ—¶')
            axes[1,0].set_ylabel('å¹³å‡ç—…æ¯’æ€§åˆ†æ•°')
            
            # ç—…æ¯’æ€§ vs é¢„æµ‹å‡†ç¡®åº¦
            if df['prediction_accuracy'].notna().any():
                axes[1,1].scatter(df['viral_score'], df['prediction_accuracy'], alpha=0.6)
                axes[1,1].set_title('ç—…æ¯’æ€§åˆ†æ•° vs é¢„æµ‹å‡†ç¡®åº¦')
                axes[1,1].set_xlabel('ç—…æ¯’æ€§åˆ†æ•°')
                axes[1,1].set_ylabel('é¢„æµ‹å‡†ç¡®åº¦')
            
            plt.tight_layout()
            plt.show()
        else:
            print("âŒ æœªæ‰¾åˆ°çˆ†æ¬¾é¢„æµ‹æ•°æ®")
        
        conn.close()
    
    def analysis_6_real_time_recommendations(self, user_id: str = None):
        """åˆ†æ6ï¼šå®æ—¶ä¸ªæ€§åŒ–æ¨è"""
        print("ğŸ” åˆ†æ6ï¼šå®æ—¶ä¸ªæ€§åŒ–æ¨èåˆ†æ")
        print("="*50)
        
        conn = self.connect_db()
        cursor = conn.cursor()
        
        if not user_id:
            # è·å–æœ€è¿‘æœ‰æ¨èçš„ç”¨æˆ·
            cursor.execute("""
                SELECT user_id FROM real_time_recommendations 
                ORDER BY generated_at DESC LIMIT 1
            """)
            result = cursor.fetchone()
            user_id = result[0] if result else "U335175"
        
        # æŸ¥è¯¢ç”¨æˆ·æ¨èæ•°æ®
        query = """
        SELECT 
            user_id,
            recommended_news,
            recommendation_scores,
            recommendation_reasons,
            context_factors,
            generated_at,
            clicked_news,
            recommendation_effectiveness
        FROM real_time_recommendations 
        WHERE user_id = %s
        ORDER BY generated_at DESC
        LIMIT 5
        """
        
        df = pd.read_sql(query, conn, params=[user_id])
        
        if not df.empty:
            print(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id}")
            print(f"ğŸ“Š æœ€è¿‘æ¨èè®°å½•:")
            
            for idx, row in df.iterrows():
                print(f"\nğŸ•’ æ¨èæ—¶é—´: {row['generated_at']}")
                
                # è§£ææ¨èæ–°é—»
                recommended_news = json.loads(row['recommended_news']) if isinstance(row['recommended_news'], str) else row['recommended_news']
                scores = json.loads(row['recommendation_scores']) if isinstance(row['recommendation_scores'], str) else row['recommendation_scores']
                
                print(f"ğŸ“° æ¨èæ–°é—»: {recommended_news[:5]}...")  # æ˜¾ç¤ºå‰5æ¡
                print(f"ğŸ“Š æ¨èåˆ†æ•°: {scores[:5] if isinstance(scores, list) else list(scores.values())[:5]}...")
                
                # æ¨èæ•ˆæœ
                if row['recommendation_effectiveness']:
                    print(f"âœ… æ¨èæ•ˆæœ: {row['recommendation_effectiveness']:.3f}")
                
                # ä¸Šä¸‹æ–‡å› ç´ 
                if row['context_factors']:
                    context = json.loads(row['context_factors']) if isinstance(row['context_factors'], str) else row['context_factors']
                    print(f"ğŸ“‹ ä¸Šä¸‹æ–‡: {context}")
            
            # æŸ¥è¯¢æ¨èæ•ˆæœç»Ÿè®¡
            effectiveness_query = """
            SELECT 
                AVG(recommendation_effectiveness) as avg_effectiveness,
                COUNT(*) as total_recommendations,
                SUM(CASE WHEN recommendation_effectiveness > 0.5 THEN 1 ELSE 0 END) as good_recommendations
            FROM real_time_recommendations 
            WHERE user_id = %s AND recommendation_effectiveness IS NOT NULL
            """
            
            cursor.execute(effectiveness_query, [user_id])
            stats = cursor.fetchone()
            
            if stats[0]:
                print(f"\nğŸ“ˆ æ¨èæ•ˆæœç»Ÿè®¡:")
                print(f"å¹³å‡æ•ˆæœ: {stats[0]:.3f}")
                print(f"æ€»æ¨èæ¬¡æ•°: {stats[1]}")
                print(f"è‰¯å¥½æ¨èç‡: {stats[2]/stats[1]*100:.1f}%")
        else:
            print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ· {user_id} çš„æ¨èæ•°æ®")
        
        conn.close()
    
    def analysis_7_query_logs(self, hours: int = 24):
        """åˆ†æ7ï¼šæŸ¥è¯¢æ—¥å¿—è®°å½•å’Œæ€§èƒ½åˆ†æ"""
        print("ğŸ” åˆ†æ7ï¼šæŸ¥è¯¢æ—¥å¿—å’Œæ€§èƒ½åˆ†æ")
        print("="*50)
        
        conn = self.connect_db()
        
        # æŸ¥è¯¢æ—¥å¿—ç»Ÿè®¡
        query = """
        SELECT 
            query_type,
            COUNT(*) as query_count,
            AVG(execution_time_ms) as avg_execution_time,
            MAX(execution_time_ms) as max_execution_time,
            MIN(execution_time_ms) as min_execution_time,
            AVG(result_count) as avg_result_count,
            SUM(CASE WHEN error_message IS NOT NULL THEN 1 ELSE 0 END) as error_count
        FROM query_logs 
        WHERE query_timestamp >= DATE_SUB(NOW(), INTERVAL %s HOUR)
        GROUP BY query_type
        ORDER BY query_count DESC
        """
        
        df = pd.read_sql(query, conn, params=[hours])
        
        if not df.empty:
            print(f"ğŸ“Š è¿‡å»{hours}å°æ—¶æŸ¥è¯¢ç»Ÿè®¡:")
            print(df.to_string(index=False))
            
            # æ€§èƒ½åˆ†æ
            total_queries = df['query_count'].sum()
            total_errors = df['error_count'].sum()
            error_rate = total_errors / total_queries * 100 if total_queries > 0 else 0
            
            print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
            print(f"æ€»æŸ¥è¯¢æ•°: {total_queries:,}")
            print(f"æ€»é”™è¯¯æ•°: {total_errors}")
            print(f"é”™è¯¯ç‡: {error_rate:.2f}%")
            print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {df['avg_execution_time'].mean():.1f}ms")
            
            # æ…¢æŸ¥è¯¢åˆ†æ
            slow_query_threshold = 1000  # 1ç§’
            slow_queries = df[df['max_execution_time'] > slow_query_threshold]
            
            if not slow_queries.empty:
                print(f"\nâš ï¸ æ…¢æŸ¥è¯¢åˆ†æ (>{slow_query_threshold}ms):")
                print(slow_queries[['query_type', 'max_execution_time', 'avg_execution_time']].to_string(index=False))
            
            # å¯è§†åŒ–
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'è¿‡å»{hours}å°æ—¶æŸ¥è¯¢æ—¥å¿—åˆ†æ', fontsize=16)
            
            # æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ
            axes[0,0].pie(df['query_count'], labels=df['query_type'], autopct='%1.1f%%')
            axes[0,0].set_title('æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ')
            
            # å¹³å‡æ‰§è¡Œæ—¶é—´å¯¹æ¯”
            axes[0,1].bar(df['query_type'], df['avg_execution_time'])
            axes[0,1].set_title('å¹³å‡æ‰§è¡Œæ—¶é—´')
            axes[0,1].set_ylabel('æ‰§è¡Œæ—¶é—´(ms)')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # é”™è¯¯ç‡åˆ†æ
            df['error_rate'] = df['error_count'] / df['query_count'] * 100
            axes[1,0].bar(df['query_type'], df['error_rate'], color='red', alpha=0.7)
            axes[1,0].set_title('å„ç±»å‹æŸ¥è¯¢é”™è¯¯ç‡')
            axes[1,0].set_ylabel('é”™è¯¯ç‡(%)')
            axes[1,0].tick_params(axis='x', rotation=45)
            
            # æŸ¥è¯¢é‡ vs æ‰§è¡Œæ—¶é—´
            axes[1,1].scatter(df['query_count'], df['avg_execution_time'], s=100, alpha=0.7)
            axes[1,1].set_title('æŸ¥è¯¢é‡ vs æ‰§è¡Œæ—¶é—´')
            axes[1,1].set_xlabel('æŸ¥è¯¢æ•°é‡')
            axes[1,1].set_ylabel('å¹³å‡æ‰§è¡Œæ—¶é—´(ms)')
            
            # æ·»åŠ æ ‡ç­¾
            for i, row in df.iterrows():
                axes[1,1].annotate(row['query_type'], 
                                 (row['query_count'], row['avg_execution_time']),
                                 xytext=(5, 5), textcoords='offset points', fontsize=8)
            
            plt.tight_layout()
            plt.show()
            
            # è¯¦ç»†çš„æ—¶é—´åºåˆ—åˆ†æ
            time_query = """
            SELECT 
                DATE_FORMAT(query_timestamp, '%%H:00') as hour,
                COUNT(*) as hourly_queries,
                AVG(execution_time_ms) as hourly_avg_time
            FROM query_logs 
            WHERE query_timestamp >= DATE_SUB(NOW(), INTERVAL %s HOUR)
            GROUP BY DATE_FORMAT(query_timestamp, '%%H:00')
            ORDER BY hour
            """
            
            time_df = pd.read_sql(time_query, conn, params=[hours])
            
            if not time_df.empty and len(time_df) > 1:
                print(f"\nâ° å°æ—¶çº§æŸ¥è¯¢è¶‹åŠ¿:")
                print(time_df.to_string(index=False))
        else:
            print(f"âŒ è¿‡å»{hours}å°æ—¶å†…æ— æŸ¥è¯¢æ—¥å¿—")
        
        conn.close()

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ‰€æœ‰ä¸ƒå¤§åˆ†æåŠŸèƒ½"""
    # MySQLé…ç½®
    mysql_config = {
        'host': 'localhost',
        'user': 'root',
        'password': 'your_password',
        'database': 'news_analytics'
    }
    
    # åˆ›å»ºåˆ†æå®ä¾‹
    analyzer = SevenAnalysisExamples(mysql_config)
    
    print("ğŸ¯ æ–°é—»å®æ—¶åˆ†æç³»ç»Ÿ - ä¸ƒå¤§åˆ†æåŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    try:
        # 1. æ–°é—»ç”Ÿå‘½å‘¨æœŸåˆ†æ
        analyzer.analysis_1_news_lifecycle()
        
        # 2. åˆ†ç±»è¶‹åŠ¿åˆ†æ
        analyzer.analysis_2_category_trends()
        
        # 3. ç”¨æˆ·å…´è¶£å˜åŒ–åˆ†æ
        analyzer.analysis_3_user_interests()
        
        # 4. å¤šç»´åº¦ç»Ÿè®¡æŸ¥è¯¢
        analyzer.analysis_4_multidim_statistics()
        
        # 5. çˆ†æ¬¾æ–°é—»é¢„æµ‹
        analyzer.analysis_5_viral_prediction()
        
        # 6. å®æ—¶æ¨è
        analyzer.analysis_6_real_time_recommendations()
        
        # 7. æŸ¥è¯¢æ—¥å¿—åˆ†æ
        analyzer.analysis_7_query_logs()
        
        print("\nâœ… æ‰€æœ‰åˆ†æåŠŸèƒ½æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main() 